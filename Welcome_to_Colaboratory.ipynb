{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lukkychan/scaling_to_high/blob/main/Welcome_to_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir original_video\n",
        "!mkdir org_no_sound\n",
        "!mkdir -p frames/1080p\n",
        "!mkdir -p frames/360p\n",
        "!mkdir scaled\n",
        "!mkdir predicted\n",
        "!mkdir Pre_orginal\n",
        "!mkdir dataset\n",
        "!mkdir -p dataset/1080p\n",
        "!mkdir -p dataset/360s"
      ],
      "metadata": {
        "id": "b9snVJi30Ge1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.youtube.com/watch?v=eDTEiIC0_rE&pp=ygUSc3B5IHRyYWlsZXIgdGVsdWd1"
      ],
      "metadata": {
        "id": "oSal6RfTJJ8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the videos"
      ],
      "metadata": {
        "id": "vMeI6f4UxaYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O \"1080p.mp4\" \"https://cdn6.savetube.me/media/eDTEiIC0_rE/spy-trailer-telugu-nikhil-siddharth-iswarya-menon-garry-bh-charantej-uppalapati-1080-ytshorts.savetube.me.mp4\""
      ],
      "metadata": {
        "id": "z3FW3-UDxYSh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ae28f33-2001-4435-d9f1-3d43ce1947d1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-30 07:09:44--  https://cdn6.savetube.me/media/eDTEiIC0_rE/spy-trailer-telugu-nikhil-siddharth-iswarya-menon-garry-bh-charantej-uppalapati-1080-ytshorts.savetube.me.mp4\n",
            "Resolving cdn6.savetube.me (cdn6.savetube.me)... 139.99.45.74\n",
            "Connecting to cdn6.savetube.me (cdn6.savetube.me)|139.99.45.74|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19116714 (18M) [video/mp4]\n",
            "Saving to: ‘1080p.mp4’\n",
            "\n",
            "1080p.mp4           100%[===================>]  18.23M  7.41MB/s    in 2.5s    \n",
            "\n",
            "2023-06-30 07:09:49 (7.41 MB/s) - ‘1080p.mp4’ saved [19116714/19116714]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O \"360p.mp4\" \"https://rr1---sn-npoe7nz7.googlevideo.com/videoplayback?expire=1688130533&ei=hX-eZLbxE87yrQH-15eADw&ip=139.99.45.74&id=o-ABcVHdyuIRW-l7JWY6kvjKugTdbRE9-T7o4v0aBxASIO&itag=18&source=youtube&requiressl=yes&mh=5x&mm=31%2C29&mn=sn-npoe7nz7%2Csn-npoeene6&ms=au%2Crdu&mv=m&mvi=1&pl=17&initcwndbps=117500&spc=Ul2Sq_HzubF7d7fy3t7PLZOutGLOXNY&vprv=1&svpuc=1&mime=video%2Fmp4&gir=yes&clen=7096879&ratebypass=yes&dur=124.435&lmt=1687436485346821&mt=1688108497&fvip=1&fexp=24007246&beids=24350017&c=ANDROID&txp=6319224&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cgir%2Cclen%2Cratebypass%2Cdur%2Clmt&sig=AOq0QJ8wRQIgL-4zeBPHW9PqAc_zrhsbhQYsUkKbFMVnNH7HOtpGOewCIQCICZ2yXFVmmyELjOS48SciiC2P5Dw-_gC_jDypd07tZw%3D%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Cinitcwndbps&lsig=AG3C_xAwRgIhAJAwWzU5sQ7hvvLN8AnTkqgVzNqVwzwPSUL76HrwOaU2AiEAp6yQ_l6dbTg3RErd7xaJ8uWYpEaUoyoLaRuT4pvt2Fs%3D&title=SPY%20Trailer%20(Telugu)%20|%20Nikhil%20Siddharth%20|%20%20Iswarya%20Menon%20|%20Garry%20BH%20|%20Charantej%20Uppalapati\""
      ],
      "metadata": {
        "id": "HJTW1xy5nhx7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0573fa54-e0ab-4bcd-ec14-63f1efbc50ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-30 07:09:52--  https://rr1---sn-npoe7nz7.googlevideo.com/videoplayback?expire=1688130533&ei=hX-eZLbxE87yrQH-15eADw&ip=139.99.45.74&id=o-ABcVHdyuIRW-l7JWY6kvjKugTdbRE9-T7o4v0aBxASIO&itag=18&source=youtube&requiressl=yes&mh=5x&mm=31%2C29&mn=sn-npoe7nz7%2Csn-npoeene6&ms=au%2Crdu&mv=m&mvi=1&pl=17&initcwndbps=117500&spc=Ul2Sq_HzubF7d7fy3t7PLZOutGLOXNY&vprv=1&svpuc=1&mime=video%2Fmp4&gir=yes&clen=7096879&ratebypass=yes&dur=124.435&lmt=1687436485346821&mt=1688108497&fvip=1&fexp=24007246&beids=24350017&c=ANDROID&txp=6319224&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cgir%2Cclen%2Cratebypass%2Cdur%2Clmt&sig=AOq0QJ8wRQIgL-4zeBPHW9PqAc_zrhsbhQYsUkKbFMVnNH7HOtpGOewCIQCICZ2yXFVmmyELjOS48SciiC2P5Dw-_gC_jDypd07tZw%3D%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Cinitcwndbps&lsig=AG3C_xAwRgIhAJAwWzU5sQ7hvvLN8AnTkqgVzNqVwzwPSUL76HrwOaU2AiEAp6yQ_l6dbTg3RErd7xaJ8uWYpEaUoyoLaRuT4pvt2Fs%3D&title=SPY%20Trailer%20(Telugu)%20%7C%20Nikhil%20Siddharth%20%7C%20%20Iswarya%20Menon%20%7C%20Garry%20BH%20%7C%20Charantej%20Uppalapati\n",
            "Resolving rr1---sn-npoe7nz7.googlevideo.com (rr1---sn-npoe7nz7.googlevideo.com)... 142.251.88.102, 2404:6800:4003:2a::6\n",
            "Connecting to rr1---sn-npoe7nz7.googlevideo.com (rr1---sn-npoe7nz7.googlevideo.com)|142.251.88.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://rr2---sn-qxoedn7k.googlevideo.com/videoplayback?expire=1688130533&ei=hX-eZLbxE87yrQH-15eADw&ip=139.99.45.74&id=o-ABcVHdyuIRW-l7JWY6kvjKugTdbRE9-T7o4v0aBxASIO&itag=18&source=youtube&requiressl=yes&spc=Ul2Sq_HzubF7d7fy3t7PLZOutGLOXNY&vprv=1&svpuc=1&mime=video%2Fmp4&gir=yes&clen=7096879&ratebypass=yes&dur=124.435&lmt=1687436485346821&fexp=24007246,24350017&beids=24350017&c=ANDROID&txp=6319224&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cgir%2Cclen%2Cratebypass%2Cdur%2Clmt&sig=AOq0QJ8wRQIgL-4zeBPHW9PqAc_zrhsbhQYsUkKbFMVnNH7HOtpGOewCIQCICZ2yXFVmmyELjOS48SciiC2P5Dw-_gC_jDypd07tZw%3D%3D&title=SPY%20Trailer%20(Telugu)%20%7C%20Nikhil%20Siddharth%20%7C%20%20Iswarya%20Menon%20%7C%20Garry%20BH%20%7C%20Charantej%20Uppalapati&redirect_counter=1&rm=sn-npo6y7e&req_id=1a0f75679f6ca3ee&cms_redirect=yes&cmsv=e&ipbypass=yes&mh=5x&mip=35.188.63.192&mm=31&mn=sn-qxoedn7k&ms=au&mt=1688107997&mv=u&mvi=2&pl=20&lsparams=ipbypass,mh,mip,mm,mn,ms,mv,mvi,pl&lsig=AG3C_xAwRQIgKyBVqlouWvfT2R9CnG7j_oXFrpjbrmyImlmlMNImzPwCIQD-8v7uJKaxoaqZn73AakgEhUNHEdReu80kRQ3NV98TOQ%3D%3D [following]\n",
            "--2023-06-30 07:09:53--  https://rr2---sn-qxoedn7k.googlevideo.com/videoplayback?expire=1688130533&ei=hX-eZLbxE87yrQH-15eADw&ip=139.99.45.74&id=o-ABcVHdyuIRW-l7JWY6kvjKugTdbRE9-T7o4v0aBxASIO&itag=18&source=youtube&requiressl=yes&spc=Ul2Sq_HzubF7d7fy3t7PLZOutGLOXNY&vprv=1&svpuc=1&mime=video%2Fmp4&gir=yes&clen=7096879&ratebypass=yes&dur=124.435&lmt=1687436485346821&fexp=24007246,24350017&beids=24350017&c=ANDROID&txp=6319224&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cgir%2Cclen%2Cratebypass%2Cdur%2Clmt&sig=AOq0QJ8wRQIgL-4zeBPHW9PqAc_zrhsbhQYsUkKbFMVnNH7HOtpGOewCIQCICZ2yXFVmmyELjOS48SciiC2P5Dw-_gC_jDypd07tZw%3D%3D&title=SPY%20Trailer%20(Telugu)%20%7C%20Nikhil%20Siddharth%20%7C%20%20Iswarya%20Menon%20%7C%20Garry%20BH%20%7C%20Charantej%20Uppalapati&redirect_counter=1&rm=sn-npo6y7e&req_id=1a0f75679f6ca3ee&cms_redirect=yes&cmsv=e&ipbypass=yes&mh=5x&mip=35.188.63.192&mm=31&mn=sn-qxoedn7k&ms=au&mt=1688107997&mv=u&mvi=2&pl=20&lsparams=ipbypass,mh,mip,mm,mn,ms,mv,mvi,pl&lsig=AG3C_xAwRQIgKyBVqlouWvfT2R9CnG7j_oXFrpjbrmyImlmlMNImzPwCIQD-8v7uJKaxoaqZn73AakgEhUNHEdReu80kRQ3NV98TOQ%3D%3D\n",
            "Resolving rr2---sn-qxoedn7k.googlevideo.com (rr2---sn-qxoedn7k.googlevideo.com)... 74.125.159.199, 2607:f8b0:400f:7::7\n",
            "Connecting to rr2---sn-qxoedn7k.googlevideo.com (rr2---sn-qxoedn7k.googlevideo.com)|74.125.159.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7096879 (6.8M) [video/mp4]\n",
            "Saving to: ‘360p.mp4’\n",
            "\n",
            "360p.mp4            100%[===================>]   6.77M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-06-30 07:09:53 (64.7 MB/s) - ‘360p.mp4’ saved [7096879/7096879]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AOdvt9xnIKlK",
        "outputId": "517fa421-c8bd-40d9-a7f5-67b1b26bea64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir '/content/drive/MyDrive/model'"
      ],
      "metadata": {
        "id": "N_VqxXeYJGeH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/org_no_sound/360p.mp4'  # Specify the path to your file\n",
        "drive_path = '/content/drive/MyDrive/model/360p.mp4'  # Specify the destination path in Google Drive\n",
        "\n",
        "# Copy the file to Google Drive\n",
        "!cp $file_path $drive_path"
      ],
      "metadata": {
        "id": "OJBoykOfIS6-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv \"1080p.mp4\" \"/content/original_video/\"\n",
        "!mv \"360p.mp4\" \"/content/original_video/\""
      ],
      "metadata": {
        "id": "veSDYI8cnsVO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing the audios"
      ],
      "metadata": {
        "id": "xJVOytvJy301"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install moviepy"
      ],
      "metadata": {
        "id": "ZZnAZuLUy_kF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a68b41f-3f7b-42c0-8444-baef2646ee4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.27.1)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.22.4)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.25.1)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.8)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (8.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "def remove_audio(input_video_path, output_video_path):\n",
        "    # Load the video clip\n",
        "    video = VideoFileClip(input_video_path)\n",
        "\n",
        "    # Remove the audio\n",
        "    video_without_audio = video.without_audio()\n",
        "\n",
        "    # Save the video without audio\n",
        "    video_without_audio.write_videofile(output_video_path, codec='libx264')\n",
        "\n",
        "    # Close the video clip\n",
        "    video.close()\n",
        "\n",
        "# Specify the paths to the input video and desired output video\n",
        "input_video_path = '/content/original_video/1080p.mp4'\n",
        "output_video_path = '/content/org_no_sound/1080p.mp4'\n",
        "\n",
        "# Call the function to remove audio\n",
        "remove_audio(input_video_path, output_video_path)\n",
        "print(\"done.\")\n"
      ],
      "metadata": {
        "id": "VeEXxSqcKAI7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf7fe5e-2d70-4e0f-b90f-443e242ff87f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video /content/org_no_sound/1080p.mp4.\n",
            "Moviepy - Writing video /content/org_no_sound/1080p.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "t: 100%|█████████▉| 2984/2987 [06:53<00:00, 28.04it/s, now=None]WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /content/original_video/1080p.mp4, 6220800 bytes wanted but 0 bytes read,at frame 2985/2987, at time 124.38/124.44 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /content/original_video/1080p.mp4, 6220800 bytes wanted but 0 bytes read,at frame 2986/2987, at time 124.42/124.44 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/org_no_sound/1080p.mp4\n",
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "def remove_audio(input_video_path, output_video_path):\n",
        "    # Load the video clip\n",
        "    video = VideoFileClip(input_video_path)\n",
        "\n",
        "    # Remove the audio\n",
        "    video_without_audio = video.without_audio()\n",
        "\n",
        "    # Save the video without audio\n",
        "    video_without_audio.write_videofile(output_video_path, codec='libx264')\n",
        "\n",
        "    # Close the video clip\n",
        "    video.close()\n",
        "\n",
        "# Specify the paths to the input video and desired output video\n",
        "input_video_path = '/content/original_video/360p.mp4'\n",
        "output_video_path = '/content/org_no_sound/360p.mp4'\n",
        "\n",
        "# Call the function to remove audio\n",
        "remove_audio(input_video_path, output_video_path)\n",
        "print(\"done.\")\n"
      ],
      "metadata": {
        "id": "RVdzq5Mmy6u-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aab8ea56-00f3-47e4-f9e7-c2039fc5e6c1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video /content/org_no_sound/360p.mp4.\n",
            "Moviepy - Writing video /content/org_no_sound/360p.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "t: 100%|█████████▉| 2978/2987 [00:46<00:00, 205.44it/s, now=None]WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /content/original_video/360p.mp4, 691200 bytes wanted but 0 bytes read,at frame 2985/2987, at time 124.38/124.44 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /content/original_video/360p.mp4, 691200 bytes wanted but 0 bytes read,at frame 2986/2987, at time 124.42/124.44 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/org_no_sound/360p.mp4\n",
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Frames"
      ],
      "metadata": {
        "id": "wwotNercz_mD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Specify the path to the video file\n",
        "video_path = \"/content/drive/MyDrive/model/360p.mp4\"\n",
        "\n",
        "# Specify the output folder to save the frames\n",
        "output_folder = \"/content/frames/360p/\"\n",
        "\n",
        "# Specify the maximum number of frames to extract\n",
        "max_frames = 150\n",
        "\n",
        "# Open the video file\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get the total number of frames in the video\n",
        "total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "print(total_frames)\n",
        "\n",
        "# Calculate the frame interval to evenly sample the frames\n",
        "frame_interval = max(total_frames // max_frames, 1)\n",
        "\n",
        "# Initialize a counter to keep track of the extracted frames\n",
        "frame_count = 0\n",
        "\n",
        "# Loop through the frames and extract the desired number of frames\n",
        "while frame_count < total_frames:\n",
        "    # Read the current frame\n",
        "    ret, frame = video.read()\n",
        "\n",
        "    # Check if the frame was successfully read\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Save the frame as an image file\n",
        "    frame_path = f\"{output_folder}{frame_count:03d}.jpg\"\n",
        "    cv2.imwrite(frame_path, frame)\n",
        "\n",
        "    # Increment the frame count\n",
        "    frame_count += 1\n",
        "\n",
        "    # Move to the next frame based on the frame interval\n",
        "    video.set(cv2.CAP_PROP_POS_FRAMES, frame_count * frame_interval)\n",
        "\n",
        "# Release the video capture object\n",
        "video.release()\n",
        "print(\"done.\")"
      ],
      "metadata": {
        "id": "biGN7KYh0DND",
        "outputId": "e0f99b11-736f-4870-fddc-a712c3f6fc25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2987\n",
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Specify the path to the video file\n",
        "video_path = \"/content/drive/MyDrive/model/1080p.mp4\"\n",
        "\n",
        "# Specify the output folder to save the frames\n",
        "output_folder = \"/content/frames/1080p/\"\n",
        "\n",
        "# Specify the maximum number of frames to extract\n",
        "max_frames = 150\n",
        "\n",
        "# Open the video file\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get the total number of frames in the video\n",
        "total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "# Calculate the frame interval to evenly sample the frames\n",
        "frame_interval = max(total_frames // max_frames, 1)\n",
        "\n",
        "# Initialize a counter to keep track of the extracted frames\n",
        "frame_count = 0\n",
        "\n",
        "# Loop through the frames and extract the desired number of frames\n",
        "while frame_count < total_frames:\n",
        "    # Read the current frame\n",
        "    ret, frame = video.read()\n",
        "\n",
        "    # Check if the frame was successfully read\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Save the frame as an image file\n",
        "    frame_path = f\"{output_folder}{frame_count:03d}.jpg\"\n",
        "    cv2.imwrite(frame_path, frame)\n",
        "\n",
        "\n",
        "    # Increment the frame count\n",
        "    frame_count += 1\n",
        "\n",
        "    # Move to the next frame based on the frame interval\n",
        "    video.set(cv2.CAP_PROP_POS_FRAMES, frame_count * frame_interval)\n",
        "\n",
        "# Release the video capture object\n",
        "video.release()\n",
        "print(\"done.\")"
      ],
      "metadata": {
        "id": "wxI4cUVpKIbd",
        "outputId": "f5dcd89e-9784-4d0d-f142-baf932f1d98d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling the images to 1080p"
      ],
      "metadata": {
        "id": "teV6as_L3czN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "def scale_images(input_folder, output_folder):\n",
        "    # Create the output folder if it doesn't exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Get a list of all image files in the input folder\n",
        "    image_files = [f for f in os.listdir(input_folder) if os.path.isfile(os.path.join(input_folder, f))]\n",
        "\n",
        "    for image_file in image_files:\n",
        "        # Construct the input and output file paths\n",
        "        input_path = os.path.join(input_folder, image_file)\n",
        "        output_path = os.path.join(output_folder, image_file)\n",
        "\n",
        "        # Execute FFmpeg command to scale the image to 1920x1080\n",
        "        command = ['ffmpeg', '-i', input_path, '-vf', 'scale=1920:1080', output_path]\n",
        "        subprocess.call(command)\n",
        "\n",
        "# Specify the paths to the input folder and output folder\n",
        "input_folder = '/content/frames/360p/'\n",
        "output_folder = '/content/scaled/'\n",
        "\n",
        "# Call the function to scale the images\n",
        "scale_images(input_folder, output_folder)\n",
        "print(\"scaling done.\")\n"
      ],
      "metadata": {
        "id": "GFatlVKr3cSN",
        "outputId": "acb4f1ca-2fe1-4a29-ae73-8bdee9ae511e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scaling done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "# Your program code here\n",
        "\n",
        "# Perform memory cleanup\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "UPxRLk6tkZDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2f5917d-f4f5-4008-dde9-1274b494f0ba"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final splitted dataset"
      ],
      "metadata": {
        "id": "vskLzaqZhrgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1080p\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "# Specify the directory containing the original 1080p images\n",
        "original_dir = '/content/frames/1080p/'\n",
        "\n",
        "# Create a directory to save the cropped images\n",
        "#os.makedirs('path/to/save/cropped/images', exist_ok=True)\n",
        "counter = 1\n",
        "# Iterate over the images in the original directory\n",
        "for filename in os.listdir(original_dir):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):  # Adjust file extensions if needed\n",
        "        # Load the original 1080p image\n",
        "        original_path = os.path.join(original_dir, filename)\n",
        "        original_image = cv2.imread(original_path)\n",
        "\n",
        "        # Calculate the dimensions of each cropped part\n",
        "        image_height, image_width, _ = original_image.shape\n",
        "        crop_height = image_height // 3\n",
        "        crop_width = image_width // 3\n",
        "\n",
        "        # Crop the image into 9 equal parts and save them in ascending order\n",
        "\n",
        "        for i in range(3):\n",
        "            for j in range(3):\n",
        "                start_x = j * crop_width\n",
        "                start_y = i * crop_height\n",
        "                end_x = start_x + crop_width\n",
        "                end_y = start_y + crop_height\n",
        "\n",
        "                cropped_image = original_image[start_y:end_y, start_x:end_x]\n",
        "\n",
        "                # Save the cropped image\n",
        "                save_path = os.path.join('/content/dataset/1080p/', f'{counter:03d}.jpg')\n",
        "                cv2.imwrite(save_path, cropped_image)\n",
        "\n",
        "                counter += 1\n",
        "\n",
        "\n",
        "# Print the size of the cropped image\n",
        "cropped_height, cropped_width, _ = cropped_image.shape\n",
        "print(f\"Cropped Image {counter} ({filename}) size: {cropped_width}x{cropped_height}\")\n",
        "print(counter)\n"
      ],
      "metadata": {
        "id": "i9RC-rvZhunY",
        "outputId": "b0a31e7b-74ba-47a9-a2da-eb9a0c64b65d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cropped Image 1423 (032.jpg) size: 640x360\n",
            "1423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#360p\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "# Specify the directory containing the original scaled 360p to 1080p images\n",
        "original_dir = '/content/scaled/'\n",
        "\n",
        "# Create a directory to save the cropped images\n",
        "#os.makedirs('path/to/save/cropped/images', exist_ok=True)\n",
        "counter = 1\n",
        "# Iterate over the images in the original directory\n",
        "for filename in os.listdir(original_dir):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):  # Adjust file extensions if needed\n",
        "        # Load the original 1080p image\n",
        "        original_path = os.path.join(original_dir, filename)\n",
        "        original_image = cv2.imread(original_path)\n",
        "\n",
        "        # Calculate the dimensions of each cropped part\n",
        "        image_height, image_width, _ = original_image.shape\n",
        "        crop_height = image_height // 3\n",
        "        crop_width = image_width // 3\n",
        "\n",
        "        # Crop the image into 9 equal parts and save them in ascending order\n",
        "        for i in range(3):\n",
        "            for j in range(3):\n",
        "                start_x = j * crop_width\n",
        "                start_y = i * crop_height\n",
        "                end_x = start_x + crop_width\n",
        "                end_y = start_y + crop_height\n",
        "\n",
        "                cropped_image = original_image[start_y:end_y, start_x:end_x]\n",
        "\n",
        "                # Save the cropped image\n",
        "                save_path = os.path.join('/content/dataset/360s/', f'{counter:03d}.jpg')\n",
        "                cv2.imwrite(save_path, cropped_image)\n",
        "\n",
        "                counter += 1\n",
        "\n",
        "# Print the size of the cropped image\n",
        "cropped_height, cropped_width, _ = cropped_image.shape\n",
        "print(f\"Cropped Image {counter} ({filename}) size: {cropped_width}x{cropped_height}\")\n",
        "print(counter)\n"
      ],
      "metadata": {
        "id": "cVfHdgUAhyLE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02741deb-6a41-4b25-a2d9-9c6120e022c2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cropped Image 1423 (032.jpg) size: 640x360\n",
            "1423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model training"
      ],
      "metadata": {
        "id": "QvfUC2Vq4F0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
        "\n",
        "\n",
        "# Step 1: Organize data and directories\n",
        "original_dir = '/content/dataset/1080p/'  # Directory containing original 1080p images\n",
        "scaled_dir = '/content/dataset/360s/'  # Directory containing scaled images\n",
        "\n",
        "# Step 2: Load and preprocess the images\n",
        "original_images = []\n",
        "scaled_images = []\n",
        "\n",
        "for filename in os.listdir(original_dir):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):  # Adjust file extensions if needed\n",
        "        original_path = os.path.join(original_dir, filename)\n",
        "        scaled_path = os.path.join(scaled_dir, filename)\n",
        "\n",
        "        original_image = cv2.imread(original_path)\n",
        "        scaled_image = cv2.imread(scaled_path)\n",
        "\n",
        "\n",
        "       # Preprocess the images (resize and normalize)\n",
        "        original_image = original_image / 255.0  # Resize to 1080p\n",
        "        scaled_image = scaled_image / 255.0  # Resize to 1080p\n",
        "\n",
        "        original_images.append(original_image)\n",
        "        scaled_images.append(scaled_image)\n",
        "\n",
        "original_images = np.array(original_images)\n",
        "scaled_images = np.array(scaled_images)\n",
        "\n",
        "# # Step 3: Define the U-Net model architecture\n",
        "inputs = Input(shape=(360, 640, 3))  # Update input shape\n",
        "conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
        "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
        "conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
        "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
        "conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
        "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
        "conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
        "up5 = UpSampling2D(size=(2, 2))(conv4)\n",
        "\n",
        "up5 = Conv2D(256, 2, activation='relu', padding='same')(up5)\n",
        "merge5 = concatenate([conv3, up5], axis=3)\n",
        "conv5 = Conv2D(256, 3, activation='relu', padding='same')(merge5)\n",
        "conv5 = Conv2D(256, 3, activation='relu', padding='same')(conv5)\n",
        "up6 = UpSampling2D(size=(2, 2))(conv5)\n",
        "\n",
        "up6 = Conv2D(128, 2, activation='relu', padding='same')(up6)\n",
        "merge6 = concatenate([conv2, up6], axis=3)\n",
        "conv6 = Conv2D(128, 3, activation='relu', padding='same')(merge6)\n",
        "conv6 = Conv2D(128, 3, activation='relu', padding='same')(conv6)\n",
        "up7 = UpSampling2D(size=(2, 2))(conv6)\n",
        "\n",
        "up7 = Conv2D(64, 2, activation='relu', padding='same')(up7)\n",
        "merge7 = concatenate([conv1, up7], axis=3)\n",
        "conv7 = Conv2D(64, 3, activation='relu', padding='same')(merge7)\n",
        "conv7 = Conv2D(64, 3, activation='relu', padding='same')(conv7)\n",
        "\n",
        "output = Conv2D(3, 1, activation='sigmoid')(conv7)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "# Step 4: Compile and train the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(scaled_images, original_images, batch_size=2, epochs=1, validation_split=0.2)\n",
        "\n",
        "# Step 5: Save the trained model\n",
        "model.save('/content/model.h5')\n",
        "print(\"model done.\")\n",
        "\n",
        "# Perform memory cleanup\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "mak56DIk4Cjs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43aff00f-7be1-448f-89d5-011eec70c028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 10/180 [>.............................] - ETA: 2:04:18 - loss: 0.0862"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2nd type model training"
      ],
      "metadata": {
        "id": "rroKuuRpB6f_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, UpSampling2D\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Step 1: Organize data and directories\n",
        "original_dir = '/content/dataset/1080p/'  # Directory containing original 1080p images\n",
        "scaled_dir = '/content/dataset/360s/'  # Directory containing scaled images\n",
        "\n",
        "# Step 2: Load and preprocess the images\n",
        "original_images = []\n",
        "scaled_images = []\n",
        "\n",
        "for filename in os.listdir(original_dir):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):  # Adjust file extensions if needed\n",
        "        original_path = os.path.join(original_dir, filename)\n",
        "        scaled_path = os.path.join(scaled_dir, filename)\n",
        "\n",
        "        original_image = cv2.imread(original_path)\n",
        "        scaled_image = cv2.imread(scaled_path)\n",
        "\n",
        "        # Preprocess the images (resize and normalize)\n",
        "        original_image = original_image / 255.0  # Resize to match scaled image dimensions\n",
        "        scaled_image = scaled_image / 255.0  # Resize to 1080p\n",
        "\n",
        "        original_images.append(original_image)\n",
        "        scaled_images.append(scaled_image)\n",
        "\n",
        "original_images = np.array(original_images)\n",
        "scaled_images = np.array(scaled_images)\n",
        "\n",
        "# Step 3: Define the model architecture\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(360, 640, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(3, activation='sigmoid'))\n",
        "\n",
        "# Step 4: Compile and train the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(scaled_images, original_images, batch_size=2, epochs=5, validation_split=0.2)\n",
        "\n",
        "# Step 5: Save the trained model\n",
        "model.save('/content/model.h5')\n",
        "\n",
        "# Step 6: Evaluate the model (optional)\n",
        "# test_images = [...]  # Load test images\n",
        "# predicted_images = model.predict(test_images)\n",
        "\n",
        "# Step 7: Inference on new scaled images (optional)\n",
        "# new_scaled_image = cv2.imread('path/to/new/scaled/image.jpg')\n",
        "# new_scaled_image = cv2.resize(new_scaled_image, (256, 256)) / 255.0\n",
        "# predicted_image = model.predict(np.array([new_scaled_image]))\n",
        "# predicted_image = predicted_image[0] * 255.0\n",
        "# cv2.imwrite('path/to/save/predicted/image.jpg', predicted_image)\n"
      ],
      "metadata": {
        "id": "pz-Opr4pB-U-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "# Your program code here\n",
        "\n",
        "# Perform memory cleanup\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "IBqALelDkNVE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fd65451-4169-43b1-9a1a-cd3700a21e17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print('CUDA is available.')\n",
        "else:\n",
        "    print('CUDA is not available.')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)\n"
      ],
      "metadata": {
        "id": "DNRVoQ665lIl",
        "outputId": "96ba48a2-c90d-4db5-bc42-fff6428a497d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available.\n",
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3rd type training with tensor"
      ],
      "metadata": {
        "id": "r3A6dVnv_sz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor, ToPILImage, RandomRotation, RandomHorizontalFlip, RandomVerticalFlip\n",
        "from torchvision.transforms import Normalize\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "# Step 1: Organize data and directories\n",
        "original_dir = '/content/dataset/1080p/'  # Directory containing original 1080p images\n",
        "scaled_dir = '/content/dataset/360s/'  # Directory containing scaled images\n",
        "\n",
        "# Step 2: Load and preprocess the images\n",
        "original_images = []\n",
        "scaled_images = []\n",
        "\n",
        "for filename in os.listdir(original_dir):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):  # Adjust file extensions if needed\n",
        "        original_path = os.path.join(original_dir, filename)\n",
        "        scaled_path = os.path.join(scaled_dir, filename)\n",
        "\n",
        "        original_image = cv2.imread(original_path)\n",
        "        scaled_image = cv2.imread(scaled_path)\n",
        "\n",
        "        # Preprocess the images (normalize)\n",
        "        original_image = original_image.astype(np.float32) / 255.0\n",
        "        scaled_image = scaled_image.astype(np.float32) / 255.0\n",
        "\n",
        "        original_images.append(original_image)\n",
        "        scaled_images.append(scaled_image)\n",
        "\n",
        "original_images = np.array(original_images)\n",
        "scaled_images = np.array(scaled_images)\n",
        "\n",
        "original_images = original_images.transpose((0, 3, 1, 2))  # Transpose from (num_samples, 640, 360, 3) to (num_samples, 3, 640, 360)\n",
        "scaled_images = scaled_images.transpose((0, 3, 1, 2))  # Transpose from (num_samples, 640, 360, 3) to (num_samples, 3, 640, 360)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda') #torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Convert the input data to the correct shape and move to GPU\n",
        "original_images = torch.from_numpy(original_images).to(device).float()\n",
        "scaled_images = torch.from_numpy(scaled_images).to(device).float()\n",
        "\n",
        "# Define the U-Net model architecture\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.encoder2 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.encoder3 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.encoder4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder4 = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        )\n",
        "        self.decoder3 = nn.Sequential(\n",
        "            nn.Conv2d(512, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        )\n",
        "        self.decoder2 = nn.Sequential(\n",
        "            nn.Conv2d(256, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        )\n",
        "        self.decoder1 = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 3, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x1 = self.encoder1(x)\n",
        "        x2 = self.encoder2(x1)\n",
        "        x3 = self.encoder3(x2)\n",
        "        x4 = self.encoder4(x3)\n",
        "\n",
        "        # Decoder\n",
        "        d4 = self.decoder4(x4)\n",
        "        d3 = self.decoder3(torch.cat([d4, x3], dim=1))\n",
        "        d2 = self.decoder2(torch.cat([d3, x2], dim=1))\n",
        "        d1 = self.decoder1(torch.cat([d2, x1], dim=1))\n",
        "\n",
        "        return d1\n",
        "\n",
        "\n",
        "# Create an instance of the U-Net model\n",
        "model = UNet()\n",
        "\n",
        "# Move the model to GPU if available\n",
        "model.to(device)\n",
        "\n",
        "# Print the model architecture\n",
        "print(model)\n",
        "\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the data augmentation and normalization transformations\n",
        "transform = nn.Sequential(\n",
        "    RandomRotation(degrees=15),\n",
        "    RandomHorizontalFlip()\n",
        ")\n",
        "\n",
        "# Define the dataset class\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, original_images, scaled_images, transform=None):\n",
        "        self.original_images = original_images\n",
        "        self.scaled_images = scaled_images\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.original_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        original_image = self.original_images[idx]\n",
        "        scaled_image = self.scaled_images[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            original_image = self.transform(original_image)\n",
        "            scaled_image = self.transform(scaled_image)\n",
        "\n",
        "        return original_image, scaled_image\n",
        "\n",
        "# Create the dataset object\n",
        "dataset = ImageDataset(original_images, scaled_images, transform)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create the data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 15\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for original_images, scaled_images in train_loader:\n",
        "        optimizer.zero_grad()  # Clear the gradients\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(original_images)\n",
        "        loss = criterion(outputs, scaled_images)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Compute the average loss for the epoch\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for original_images, scaled_images in val_loader:\n",
        "            outputs = model(original_images)\n",
        "            loss = criterion(outputs, scaled_images)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    # Print the epoch statistics\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'model.pth')\n",
        "\n",
        "print(\"model saved.\")\n",
        "\n",
        "# Clear GPU memory\n",
        "del original_images, scaled_images, model, criterion, optimizer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "# End the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the execution time\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "print(f\"Execution time: {execution_time/60} minutes.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "46FJeaUn_vn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.o prediction"
      ],
      "metadata": {
        "id": "xZWocIUTNnQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Load the trained model\n",
        "model = UNet()\n",
        "model.load_state_dict(torch.load('/content/model.pth'))\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Load and preprocess the input image\n",
        "input_image = cv2.imread('/content/dataset/360s/045.jpg')\n",
        "input_image = cv2.resize(input_image, (640, 360))\n",
        "input_tensor = ToTensor()(input_image).unsqueeze(0).to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    # Forward pass\n",
        "    output_tensor = model(input_tensor)\n",
        "\n",
        "# Convert the output tensor to a numpy array\n",
        "output_image = output_tensor.squeeze(0).cpu().numpy()\n",
        "\n",
        "# Convert the output tensor to a numpy array\n",
        "output_image = output_tensor.squeeze(0).cpu().numpy()\n",
        "\n",
        "# Transpose the output array to match the shape (height, width, channels)\n",
        "output_image = np.transpose(output_image, (1, 2, 0))\n",
        "\n",
        "# Scale the pixel values from [0, 1] to [0, 255]\n",
        "output_image = (output_image * 255).astype(np.uint8)\n",
        "\n",
        "# Convert the output array to a PIL image\n",
        "output_image = Image.fromarray(output_image)\n",
        "\n",
        "# Save the predicted image\n",
        "output_image.save('/content/predicted/predicted_image.jpg')\n"
      ],
      "metadata": {
        "id": "N33Lvsk6Nrk7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.o comparing"
      ],
      "metadata": {
        "id": "JS5Pa4WQPL1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Load the original and predicted images\n",
        "original_image = Image.open('/content/dataset/1080p/045.jpg')\n",
        "predicted_image = Image.open('/content/predicted/predicted_image.jpg')\n",
        "\n",
        "# Convert images to numpy arrays\n",
        "original_array = np.array(original_image)\n",
        "predicted_array = np.array(predicted_image)\n",
        "\n",
        "# Calculate the pixel-wise difference\n",
        "diff_array = np.abs(original_array - predicted_array)\n",
        "\n",
        "# Calculate the number of mismatched pixels\n",
        "num_mismatched_pixels = np.sum(diff_array > 0)\n",
        "\n",
        "# Calculate the total number of pixels\n",
        "total_pixels = original_array.size\n",
        "\n",
        "# Calculate the accuracy as the percentage of matching pixels\n",
        "accuracy = ((total_pixels - num_mismatched_pixels) / total_pixels) * 100\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llYc24HQPLQj",
        "outputId": "dd2d196b-b88b-404c-8fd5-fe7b7266300e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.16%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 model training"
      ],
      "metadata": {
        "id": "PpbrDQROK-ZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.transforms import ToPILImage\n",
        "\n",
        "\n",
        "# Step 1: Organize data and directories\n",
        "original_dir = '/content/dataset/1080p/'  # Directory containing original 1080p images\n",
        "scaled_dir = '/content/dataset/360s/'  # Directory containing scaled images\n",
        "\n",
        "# Step 2: Load and preprocess the images\n",
        "original_images = []\n",
        "scaled_images = []\n",
        "\n",
        "for filename in os.listdir(original_dir):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):  # Adjust file extensions if needed\n",
        "        original_path = os.path.join(original_dir, filename)\n",
        "        scaled_path = os.path.join(scaled_dir, filename)\n",
        "\n",
        "        original_image = cv2.imread(original_path)\n",
        "        scaled_image = cv2.imread(scaled_path)\n",
        "\n",
        "        # Preprocess the images (resize and normalize)\n",
        "        original_image = cv2.resize(original_image, (640, 360))\n",
        "        scaled_image = cv2.resize(scaled_image, (640, 360))\n",
        "\n",
        "        original_images.append(original_image)\n",
        "        scaled_images.append(scaled_image)\n",
        "\n",
        "original_images = np.array(original_images)\n",
        "scaled_images = np.array(scaled_images)\n",
        "\n",
        "original_images = original_images.transpose((0, 3, 1, 2))  # Transpose from (num_samples, 640, 360, 3) to (num_samples, 3, 640, 360)\n",
        "scaled_images = scaled_images.transpose((0, 3, 1, 2))  # Transpose from (num_samples, 640, 360, 3) to (num_samples, 3, 640, 360)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Convert the input data to the correct shape and move to GPU\n",
        "original_images = torch.from_numpy(original_images).to(device).float()\n",
        "scaled_images = torch.from_numpy(scaled_images).to(device).float()\n",
        "\n",
        "# Define the U-Net model architecture\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.encoder2 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.encoder3 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.encoder4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder4 = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        )\n",
        "        self.decoder3 = nn.Sequential(\n",
        "            nn.Conv2d(512, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        )\n",
        "        self.decoder2 = nn.Sequential(\n",
        "            nn.Conv2d(256, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        )\n",
        "        self.decoder1 = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 3, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x1 = self.encoder1(x)\n",
        "        x2 = self.encoder2(x1)\n",
        "        x3 = self.encoder3(x2)\n",
        "        x4 = self.encoder4(x3)\n",
        "\n",
        "        # Decoder\n",
        "        d4 = self.decoder4(x4)\n",
        "        d3 = self.decoder3(torch.cat([d4, x3], dim=1))\n",
        "        d2 = self.decoder2(torch.cat([d3, x2], dim=1))\n",
        "        d1 = self.decoder1(torch.cat([d2, x1], dim=1))\n",
        "\n",
        "        return d1\n",
        "\n",
        "\n",
        "# Create an instance of the U-Net model\n",
        "model = UNet()\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Print the model architecture\n",
        "print(model)\n",
        "\n",
        "# Step 5: Compile and train the model\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Define a custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, inputs, targets, transform=None):\n",
        "        self.inputs = inputs\n",
        "        self.targets = targets\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_image = self.inputs[idx]\n",
        "        target_image = self.targets[idx]\n",
        "\n",
        "        # Convert tensors to PIL images\n",
        "        input_image = ToPILImage()(input_image)\n",
        "        target_image = ToPILImage()(target_image)\n",
        "\n",
        "        if self.transform:\n",
        "            input_image = self.transform(input_image)\n",
        "            target_image = self.transform(target_image)\n",
        "\n",
        "        return input_image, target_image\n",
        "\n",
        "\n",
        "# Create dataloaders for training and validation\n",
        "train_dataset = CustomDataset(scaled_images, original_images, transform=ToTensor())\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1):\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in train_dataloader:\n",
        "        inputs = inputs.to(device).float()  # Convert input data to float\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {running_loss}\")\n",
        "\n",
        "# Step 6: Save the trained model\n",
        "torch.save(model.state_dict(), '/content/model.pth')\n",
        "print(\"Model saved.\")\n",
        "\n",
        "# Perform memory cleanup\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "Muh2yrTiK-J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 prediction"
      ],
      "metadata": {
        "id": "5qo5xZm9Ln8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Load the trained model\n",
        "model = UNet()\n",
        "model.load_state_dict(torch.load('/content/model.pth'))\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Load and preprocess the input image\n",
        "input_image = cv2.imread('/content/dataset/360s/045.jpg')\n",
        "input_image = cv2.resize(input_image, (640, 360))\n",
        "input_tensor = ToTensor()(input_image).unsqueeze(0).to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    # Forward pass\n",
        "    output_tensor = model(input_tensor)\n",
        "\n",
        "# Convert the output tensor to a numpy array\n",
        "output_image = output_tensor.squeeze(0).cpu().numpy()\n",
        "\n",
        "# Convert the output tensor to a numpy array\n",
        "output_image = output_tensor.squeeze(0).cpu().numpy()\n",
        "\n",
        "# Transpose the output array to match the shape (height, width, channels)\n",
        "output_image = np.transpose(output_image, (1, 2, 0))\n",
        "\n",
        "# Scale the pixel values from [0, 1] to [0, 255]\n",
        "output_image = (output_image * 255).astype(np.uint8)\n",
        "\n",
        "# Convert the output array to a PIL image\n",
        "output_image = Image.fromarray(output_image)\n",
        "\n",
        "# Save the predicted image\n",
        "output_image.save('/content/predicted/predicted_image.jpg')\n"
      ],
      "metadata": {
        "id": "S25W3tpwLp_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 comparision"
      ],
      "metadata": {
        "id": "KygicTwgLuye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Load the original and predicted images\n",
        "original_image = Image.open('/content/dataset/1080p/045.jpg')\n",
        "predicted_image = Image.open('/content/predicted/predicted_image.jpg')\n",
        "\n",
        "# Convert images to numpy arrays\n",
        "original_array = np.array(original_image)\n",
        "predicted_array = np.array(predicted_image)\n",
        "\n",
        "# Calculate the pixel-wise difference\n",
        "diff_array = np.abs(original_array - predicted_array)\n",
        "\n",
        "# Calculate the number of mismatched pixels\n",
        "num_mismatched_pixels = np.sum(diff_array > 0)\n",
        "\n",
        "# Calculate the total number of pixels\n",
        "total_pixels = original_array.size\n",
        "\n",
        "# Calculate the accuracy as the percentage of matching pixels\n",
        "accuracy = ((total_pixels - num_mismatched_pixels) / total_pixels) * 100\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd2d196b-b88b-404c-8fd5-fe7b7266300e",
        "id": "EmfpRnTFLyEr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.16%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "saving the model to drive"
      ],
      "metadata": {
        "id": "mtUySTnQ8UDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/my_file.txt'  # Specify the path to your file\n",
        "drive_path = '/content/drive/MyDrive/my_folder/my_file.txt'  # Specify the destination path in Google Drive\n",
        "\n",
        "# Copy the file to Google Drive\n",
        "!cp $file_path $drive_path\n"
      ],
      "metadata": {
        "id": "vpyp27BR8X_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "predicting"
      ],
      "metadata": {
        "id": "kobakGIDYfs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Evaluate the model (optional)\n",
        "#test_images = \"/content/scaled/022.jpg\"  # Load test images\n",
        "#predicted_images = model.predict(test_images)\n",
        "\n",
        "# Step 7: Inference on new scaled images (optional)\n",
        "new_scaled_image = cv2.imread('/content/scaled/022.jpg')\n",
        "new_scaled_image = cv2.resize(new_scaled_image, (1920, 1080)) / 255.0  # Resize to 1080p and normalize\n",
        "predicted_image = model.predict(np.array([new_scaled_image]))\n",
        "predicted_image = predicted_image[0] * 255.0\n",
        "cv2.imwrite('/content/image.jpg', predicted_image)\n",
        "\n",
        "print(\"done.\")"
      ],
      "metadata": {
        "id": "AiabOG5cYfMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating"
      ],
      "metadata": {
        "id": "wEPwSgq6ZwTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import load_model\n",
        "from skimage.metrics import mean_squared_error, structural_similarity\n",
        "\n",
        "def compare_images(original_dir, predicted_dir):\n",
        "    mse_total = 0\n",
        "    ssim_total = 0\n",
        "    num_images = 0\n",
        "\n",
        "    for filename in os.listdir(original_dir):\n",
        "        if filename.endswith('.jpg') or filename.endswith('.png'):  # Adjust file extensions if needed\n",
        "            original_path = os.path.join(original_dir, filename)\n",
        "            predicted_path = os.path.join(predicted_dir, filename)\n",
        "\n",
        "            original_image = cv2.imread(original_path)\n",
        "            predicted_image = cv2.imread(predicted_path)\n",
        "\n",
        "            original_image = cv2.resize(original_image, (256, 256))\n",
        "            predicted_image = cv2.resize(predicted_image, (256, 256))\n",
        "\n",
        "            mse = mean_squared_error(original_image, predicted_image)\n",
        "            ssim = structural_similarity(original_image, predicted_image, multichannel=True)\n",
        "\n",
        "            mse_total += mse\n",
        "            ssim_total += ssim\n",
        "            num_images += 1\n",
        "\n",
        "    mse_avg = mse_total / num_images\n",
        "    ssim_avg = ssim_total / num_images\n",
        "\n",
        "    accuracy = 1.0 - mse_avg  # Inverse MSE is used as accuracy metric\n",
        "\n",
        "    return accuracy, mse_avg, ssim_avg\n",
        "\n",
        "# Paths to the original and predicted image directories\n",
        "original_dir = '/content/Pre_orginal/'\n",
        "predicted_dir = '/content/predicted/'\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('/content/model.h5')\n",
        "\n",
        "# Make predictions on new images and save them to the predicted directory\n",
        "# new_images = [...]  # Load new images for prediction\n",
        "# for i, image in enumerate(new_images):\n",
        "#     predicted_image = model.predict(np.array([image]))\n",
        "#     predicted_image = np.squeeze(predicted_image) * 255.0\n",
        "#     cv2.imwrite(os.path.join(predicted_dir, f'predicted_{i}.jpg'), predicted_image)\n",
        "\n",
        "# Compare the predicted images with the original images and calculate accuracy\n",
        "accuracy, mse, ssim = compare_images(original_dir, predicted_dir)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"MSE: {mse:.4f}\")\n",
        "print(f\"SSIM: {ssim:.4f}\")\n"
      ],
      "metadata": {
        "id": "nwCf_6KmcEoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "checkin that cropped image and coresponding region of cropped image in original ti know is there any pixel or quality loss"
      ],
      "metadata": {
        "id": "ZXq_mk1sfCpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O nivetha.jpg \"https://pbs.twimg.com/media/FgiNhi1aAAAm43h?format=jpg&name=large\""
      ],
      "metadata": {
        "id": "x9iR8AJ1ftHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the original image\n",
        "original_image = cv2.imread('/content/nivetha.jpg')\n",
        "\n",
        "# Define the region of interest (ROI) coordinates\n",
        "x = 100  # starting x-coordinate\n",
        "y = 200  # starting y-coordinate\n",
        "width = 300  # width of the ROI\n",
        "height = 200  # height of the ROI\n",
        "\n",
        "# Crop the image based on the ROI\n",
        "cropped_image = original_image[y:y+height, x:x+width]\n",
        "\n",
        "# Compare pixel values of the cropped image with the corresponding region in the original image\n",
        "are_same = np.array_equal(cropped_image, original_image[y:y+height, x:x+width])\n",
        "\n",
        "# Print the result\n",
        "if are_same:\n",
        "    print(\"The pixel values of the cropped image and the corresponding region in the original image are the same.\")\n",
        "else:\n",
        "    print(\"The pixel values of the cropped image and the corresponding region in the original image are different.\")\n"
      ],
      "metadata": {
        "id": "EV4v5mqpffCs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colaboratory",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}